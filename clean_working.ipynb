{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "318fef0d-8fa8-4b03-81cc-add3ba075d3d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      "Enter your OpenAI API key:  ········\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Deleted collection 'code_snippet_collection'\n",
      "Created new collection 'code_snippet_collection' at ./chroma_storage\n",
      "Initialized OpenAI embeddings\n"
     ]
    }
   ],
   "source": [
    "# Uncomment and run this cell 1st time to install dependencies\n",
    "# %pip install -qU langchain langchain-openai langchain-chroma chromadb sentence-transformers\n",
    "\n",
    "import os, getpass\n",
    "\n",
    "# Clear any env OPENAI_API_KEY (for session safety)\n",
    "os.environ.pop(\"OPENAI_API_KEY\", None)\n",
    "try:\n",
    "    os.unsetenv(\"OPENAI_API_KEY\")\n",
    "except Exception:\n",
    "    pass\n",
    "\n",
    "# Securely input your OpenAI API key interactively\n",
    "os.environ[\"OPENAI_API_KEY\"] = getpass.getpass(\"Enter your OpenAI API key: \")\n",
    "\n",
    "from langchain_openai import OpenAIEmbeddings\n",
    "import chromadb\n",
    "\n",
    "# Initialize Chroma persistent client and delete/create collection\n",
    "persist_path = \"./chroma_storage\"\n",
    "client = chromadb.PersistentClient(path=persist_path)\n",
    "\n",
    "collection_name = \"code_snippet_collection\"\n",
    "\n",
    "try:\n",
    "    client.delete_collection(collection_name)\n",
    "    print(f\"Deleted collection '{collection_name}'\")\n",
    "except Exception:\n",
    "    print(f\"No existing collection named '{collection_name}' to delete\")\n",
    "\n",
    "collection = client.create_collection(\n",
    "    name=collection_name,\n",
    "    metadata={\"hnsw:space\": \"cosine\"}  # cosine distance metric\n",
    ")\n",
    "print(f\"Created new collection '{collection_name}' at {persist_path}\")\n",
    "\n",
    "# Initialize OpenAI embeddings (model=text-embedding-3-small)\n",
    "embeddings = OpenAIEmbeddings(\n",
    "    model=\"text-embedding-3-small\",\n",
    "    request_timeout=60,\n",
    "    max_retries=8,\n",
    ")\n",
    "print(\"Initialized OpenAI embeddings\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "e16236ce-2f2e-40ba-8553-48818e880874",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded 6 code chunks\n",
      "Loaded 2 lineage chunks\n"
     ]
    }
   ],
   "source": [
    "import hashlib\n",
    "from typing import List\n",
    "\n",
    "def load_file_chunks(filepath: str, chunk_size=500) -> List[str]:\n",
    "    with open(filepath, \"r\", encoding=\"utf-8\") as f:\n",
    "        content = f.read()\n",
    "    lines = content.splitlines()\n",
    "    chunks = []\n",
    "    cur_chunk = []\n",
    "    cur_len = 0\n",
    "    for line in lines:\n",
    "        cur_chunk.append(line)\n",
    "        cur_len += len(line)\n",
    "        if cur_len > chunk_size:\n",
    "            chunks.append(\"\\n\".join(cur_chunk))\n",
    "            cur_chunk = []\n",
    "            cur_len = 0\n",
    "    if cur_chunk:\n",
    "        chunks.append(\"\\n\".join(cur_chunk))\n",
    "    return chunks\n",
    "\n",
    "# Load your actual code and lineage output files here\n",
    "pyspark_code_file = \"./lineage_test.py\"\n",
    "lineage_output_file = \"./lineage_output_clean.log\"\n",
    "\n",
    "code_chunks = load_file_chunks(pyspark_code_file)\n",
    "lineage_chunks = load_file_chunks(lineage_output_file)\n",
    "\n",
    "print(f\"Loaded {len(code_chunks)} code chunks\")\n",
    "print(f\"Loaded {len(lineage_chunks)} lineage chunks\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "acc660b3-343c-424e-b6c8-8113a9c13c32",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Upserted 6 snippets\n"
     ]
    }
   ],
   "source": [
    "# Helper to create stable ids for documents\n",
    "def make_id(text: str) -> str:\n",
    "    return hashlib.sha256(text.encode(\"utf-8\")).hexdigest()\n",
    "\n",
    "def upsert_snippets(snippets: List[str]) -> int:\n",
    "    ids = [make_id(s) for s in snippets]\n",
    "    vectors = embeddings.embed_documents(snippets)\n",
    "    collection.upsert(documents=snippets, embeddings=vectors, ids=ids)\n",
    "    print(f\"Upserted {len(ids)} snippets\")\n",
    "    return len(ids)\n",
    "\n",
    "# Only index code snippets (lineage will be dynamically summarized later)\n",
    "num_upserted = upsert_snippets(code_chunks)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "6066a015-cb2f-4cc4-a885-bd10c392a0ef",
   "metadata": {},
   "outputs": [],
   "source": [
    "#!uv pip install langchain[community]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "9b343029-bd5c-4367-8b4f-31046503b8ab",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_13759/873847178.py:41: LangChainDeprecationWarning: The class `LLMChain` was deprecated in LangChain 0.1.17 and will be removed in 1.0. Use :meth:`~RunnableSequence, e.g., `prompt | llm`` instead.\n",
      "  summary_chain = LLMChain(llm=llm, prompt=prompt, output_key=\"summary\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== Focused Lineage Summary ===\n",
      "The 'discounted_amount' is calculated through a series of transformations. Initially, two datasets are joined on the 'order_id' attribute. Then, a new attribute 'discounted_amount' is created using a 'CaseWhen' function. This function checks if the 'tier' attribute is equal to 'Premium'. If it is, it multiplies the 'amount' attribute by 0.9, effectively applying a 10% discount. If the 'tier' is not 'Premium', the original 'amount' is retained. This operation results in a new column 'discounted_amount'. The data is then grouped by 'product' and 'tier' attributes, and several aggregations are performed, including the sum of 'discounted_amount' (named 'tier_revenue'), the average of 'discounted_amount' (named 'avg_tier_revenue'), and the sum of 'quantity' (named 'tier_quantity'). The final output is sorted in ascending order by 'product' and 'tier'.\n"
     ]
    }
   ],
   "source": [
    "from langchain_openai import ChatOpenAI\n",
    "from langchain.chains.summarize import load_summarize_chain\n",
    "from langchain.docstore.document import Document\n",
    "\n",
    "llm = ChatOpenAI(model_name=\"gpt-4\", temperature=0)\n",
    "\n",
    "# Use refine chain to handle longer lineage text progressively\n",
    "summarization_chain = load_summarize_chain(llm, chain_type=\"refine\")\n",
    "\n",
    "# Join lineage chunks into one text block\n",
    "lineage_text = \"\\n\".join(lineage_chunks)\n",
    "lineage_doc = Document(page_content=lineage_text)\n",
    "\n",
    "# Define a clear, focused prompt for summarization\n",
    "prompt_template = \"\"\"\n",
    "You are a data engineer assistant. Given the user's question and a detailed data lineage JSON, generate a concise natural language summary focused on the transformations relevant to the question.\n",
    "\n",
    "User query: {user_query}\n",
    "\n",
    "Lineage details:\n",
    "{lineage_text}\n",
    "\n",
    "Instructions:\n",
    "- Explain key columns involved (e.g., computed columns like discounted_amount).\n",
    "- Describe the transformations, joins, and filters affecting those columns.\n",
    "- Mention any aggregations or final outputs.\n",
    "- Avoid technical metadata or runtime info.\n",
    "- Keep summary under 800 characters.\n",
    "\"\"\"\n",
    "\n",
    "# Wrap LangChain chain to include a dynamic prompt\n",
    "from langchain.prompts import PromptTemplate\n",
    "\n",
    "prompt = PromptTemplate(\n",
    "    input_variables=[\"user_query\", \"lineage_text\"],\n",
    "    template=prompt_template\n",
    ")\n",
    "\n",
    "from langchain.chains import LLMChain\n",
    "\n",
    "summary_chain = LLMChain(llm=llm, prompt=prompt, output_key=\"summary\")\n",
    "\n",
    "# Run chain on lineage text and current user question (replace with actual user query)\n",
    "current_user_query = \"Explain how discounted_amount is calculated\"\n",
    "\n",
    "summary_result = summary_chain.invoke(\n",
    "    {\"user_query\": current_user_query, \"lineage_text\": lineage_text}\n",
    ")\n",
    "\n",
    "lineage_summary_text = summary_result.get(\"summary\", \"\").strip()\n",
    "\n",
    "print(\"=== Focused Lineage Summary ===\")\n",
    "print(lineage_summary_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "d984cb6e-a786-4b9f-aeaa-e5be059259f9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Combined query (partial):\n",
      "Explain how discounted_amount is calculated\n",
      "Lineage summary:\n",
      "The 'discounted_amount' is calculated through a series of transformations. Initially, two datasets are joined on the 'order_id' attribute. Then, a new attribute 'discounted_amount' is created using a 'CaseWhen' function. This function chec...\n",
      "\n",
      "Top 5 results:\n",
      "Rank 1 - Distance: 0.4485\n",
      "    )\n",
      "\n",
      "# Transformation 2: Customer Enrichment with Discounts\n",
      "enriched_df = sales_df.join(customer_df, \"order_id\", \"inner\") \\\n",
      "    .withColumn(\n",
      "        \"discounted_amount\",\n",
      "        when(col(\"tier\") == \"Premium\", col(\"amount\") * 0.9)\n",
      "        .otherwise(col(\"amount\"))\n",
      "    )\n",
      "\n",
      "# Transformation 3: Final Analytics by Product and Tier\n",
      "final_analytics_df = enriched_df.groupBy(\"product\", \"tier\") \\\n",
      "    .agg(\n",
      "        spark_sum(\"discounted_amount\").alias(\"tier_revenue\"),\n",
      "        avg(\"discounted_amount\").alias(\"avg_tier_revenue\"),\n",
      "----------------------------------------\n",
      "Rank 2 - Distance: 0.4840\n",
      "        spark_sum(\"quantity\").alias(\"tier_quantity\")\n",
      "    ) \\\n",
      "    .orderBy(\"product\", \"tier\")\n",
      "\n",
      "print(\"Transformations executed - Spline captured lineage\")\n",
      "\n",
      "print(\"📊 RESULTS (Spline captures lineage on these actions):\")\n",
      "\n",
      "print(\"\\n--- Product Revenue Summary ---\")\n",
      "product_revenue_df.show()\n",
      "\n",
      "print(\"--- Customer Enriched Data Sample ---\")\n",
      "enriched_df.select(\"product\", \"customer_name\", \"tier\", \"amount\", \"discounted_amount\").show(5)\n",
      "\n",
      "print(\"--- Final Analytics by Product and Tier ---\")\n",
      "final_analytics_df.show()\n",
      "\n",
      "# Write to parquet (Spline captures write lineage)\n",
      "----------------------------------------\n",
      "Rank 3 - Distance: 0.5488\n",
      "customer_df = spark.createDataFrame(customer_data, [\"order_id\", \"customer_name\", \"tier\"])\n",
      "\n",
      "print(\"Source DataFrames created - Spline tracking lineage\")\n",
      "\n",
      "from pyspark.sql.functions import col, sum as spark_sum, avg, when\n",
      "\n",
      "print(\"🔄 Executing Transformations...\")\n",
      "\n",
      "# Transformation 1: Product Revenue Calculation\n",
      "product_revenue_df = sales_df.groupBy(\"product\") \\\n",
      "    .agg(\n",
      "        spark_sum(\"amount\").alias(\"total_revenue\"),\n",
      "        avg(\"amount\").alias(\"avg_order_value\"),\n",
      "        spark_sum(\"quantity\").alias(\"total_quantity\")\n",
      "----------------------------------------\n",
      "Rank 4 - Distance: 0.6871\n",
      "sales_data = [\n",
      "    (1, \"Product_A\", 100, 5, \"2023-01-01\"),\n",
      "    (2, \"Product_B\", 200, 3, \"2023-01-02\"),\n",
      "    (3, \"Product_A\", 150, 2, \"2023-01-03\"),\n",
      "    (4, \"Product_C\", 300, 4, \"2023-01-04\"),\n",
      "    (5, \"Product_B\", 250, 6, \"2023-01-05\")\n",
      "]\n",
      "\n",
      "customer_data = [\n",
      "    (1, \"John\", \"Premium\"),\n",
      "    (2, \"Jane\", \"Standard\"),\n",
      "    (3, \"Bob\", \"Premium\"),\n",
      "    (4, \"Alice\", \"Standard\"),\n",
      "    (5, \"Charlie\", \"Premium\")\n",
      "]\n",
      "\n",
      "# Create DataFrames (Spline captures this)\n",
      "sales_df = spark.createDataFrame(sales_data, [\"order_id\", \"product\", \"amount\", \"quantity\", \"order_date\"])\n",
      "----------------------------------------\n",
      "Rank 5 - Distance: 0.7426\n",
      "output_path = \"./output/spline_analytics\"\n",
      "final_analytics_df.write.mode(\"overwrite\").parquet(output_path)\n",
      "print(f\"📁 Data written to {output_path} - Spline captured write lineage\")\n",
      "\n",
      "spark.stop()\n",
      "----------------------------------------\n"
     ]
    }
   ],
   "source": [
    "def query_codebase_with_focused_summary(user_query: str, lineage_summary: str, top_k: int = 5):\n",
    "    combined_query = user_query + \"\\nLineage summary:\\n\" + lineage_summary\n",
    "    query_vector = embeddings.embed_query(combined_query)\n",
    "    \n",
    "    results = collection.query(\n",
    "        query_embeddings=[query_vector],\n",
    "        n_results=top_k,\n",
    "        include=[\"documents\", \"distances\"]\n",
    "    )\n",
    "    \n",
    "    print(f\"\\nCombined query (partial):\\n{combined_query[:300]}...\\n\")\n",
    "    print(f\"Top {top_k} results:\")\n",
    "    for rank, (doc, dist) in enumerate(zip(results[\"documents\"][0], results[\"distances\"][0]), start=1):\n",
    "        print(f\"Rank {rank} - Distance: {dist:.4f}\")\n",
    "        print(doc)\n",
    "        print(\"-\" * 40)\n",
    "\n",
    "# Example usage:\n",
    "user_query = \"Explain how discounted_amount is calculated\"\n",
    "query_codebase_with_focused_summary(user_query, lineage_summary_text)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
